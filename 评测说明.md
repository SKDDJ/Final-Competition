# 比赛决赛评测说明

## 提交说明

1. 决赛提交方式与初赛相同，项目需将和比赛相关的代码、模型权重、依赖库等数据打包为镜像tar包后进行提交（注意将构建镜像的Dockerfile文件包含在打包镜像中）。文件命名为**选手团队名字_submit.tar.gz**(打包后的tar文件在linux平台使用gzip压缩，win平台使用压缩软件压缩为.gz格式)。选手创建名字为project的文件夹，将.tar.gz文件放到project文件夹下打包为project.zip后提交。（最终需要上传平台的为project.zip，请切勿直接提交.tar.gz文件）
2. 选手需要按照要求，实现 `prepare_context`和 `process_one_json`两个函数，保证输入和输出格式规范，并将这两个函数保存在workspace目录下的**load_model.py**脚本中，以适配评测采样脚本。
3. 选手需要将使用到的模型提前保存到docker中，同时注意**保证docker中加载模型的路径正确**。

## 评测标准

决赛从人脸相似度和图文匹配度两个角度进行评分。人脸相似度使用**insightface**特征的cosin相似度。图文匹配度使用**ImageReward**作为图文相似度。

要求选手仅实现 `prepare_context`和 `process_one_json`两个函数进行任务处理，评测脚本将循环提供测试集执行 `process_one_json`函数**1.5个小时**，之后将对所有完成的任务计算归一化后的累计人脸分数和文本相似度分数。

下面是对一个task中评分函数，其中：

1. ev与初赛中Evaluator()类似，其中 `get_face_embedding`获取人脸特征向量，`image_reward`是**ImageReward**模型，`score`函数计算图文匹配度；
2. source_json是评测集中一个task的json描述输入；
3. gen_json是对应任务process_one_json()采样输出的json格式描述；
4. bound_json中记录了训练集中源图片和原版unidiffuser输出图片得到人脸相似度和文本匹配度上下限，用于归一化选手的得分:

   ```
   normed_score_face = (face_score - min_face_sim) / (max_face_sim - min_face_sim)
   normed_score_image_reward = (image_reward - min_image_reward) / (max_image_reward min_image_reward)
   ```

   根据 `人脸归一化累加分数*2.5 + 文本归一化累加分数`作为最后的分数, 计算最终排名。

   **注意：只有 `normed_score_face`大于0.1, `normed_score_image_reward`大于0.1的item才会被计分, 否则以0分计算。**
5. out_json_dir是保存对应task的成绩json格式输出的文件夹。

```python
def score(ev, source_json, gen_json, bound_json, out_json_dir):
    # get ref images
    ref_image_paths = [ i["path"] for i in source_json["source_group"]]
    ref_face_embs = [ev.get_face_embedding(read_img_pil(i)) for i in ref_image_paths]
    ref_face_embs  = [emb for emb in ref_face_embs if emb is not None] # remove None
    ref_face_embs = torch.cat(ref_face_embs)

    face_ac_scores = 0
    image_reward_ac_scores = 0
    normed_face_ac_scores = 0
    normed_image_reward_ac_scores = 0
  
    out_json = {"id": gen_json["id"], "images": []}
    commom_prompts = set([item["prompt"] for item in gen_json["images"]]) & set([item["prompt"] for item in bound_json["images"]])
    prompt_to_item = {item["prompt"]: item for item in gen_json["images"]}
    bound_prompt_to_item = {item["prompt"]: item for item in bound_json["images"]}
    if len(commom_prompts) != len(bound_json["images"]):
        print(f"共有{len(commom_prompts)}个prompt, bound json有{len(bound_json['images'])}个prompt")
        print(bound_json)
  
    for prompt in commom_prompts:
        item = prompt_to_item[prompt]
        bound_item = bound_prompt_to_item[prompt]
      
        assert item["prompt"] == bound_item["prompt"], f"prompt {item['prompt']} not equal to bound prompt {bound_item['prompt']}"
        if len(item["paths"]) < 4:
            continue    
          
        # image reward
        samples = [read_img_pil(sample_path) for sample_path in item["paths"]]# read images
        scores_image_reward = [ev.image_reward.score(item["prompt"], sample_path) for sample_path in item["paths"]]
        mean_image_reward = np.mean(scores_image_reward)
              
        # face similarity
        sample_faces = [ev.get_face_embedding(sample) for sample in samples]
        sample_faces = [emb for emb in sample_faces if emb is not None] # remove None
        if len(sample_faces) <= 1:
            print("too few faces")
            continue
          
        scores_face = [(sample_face @ ref_face_embs.T).mean().item() for sample_face in sample_faces]
        mean_face = np.mean(scores_face)
      
        subed_score_face = mean_face - bound_item["min_face_sim"]
        subed_image_reward = mean_image_reward - bound_item["min_image_reward"]
      
        normed_score_face = subed_score_face / (bound_item["max_face_sim"] - bound_item["min_face_sim"])
        normed_score_image_reward = subed_image_reward / (bound_item["max_image_reward"] - bound_item["min_image_reward"])

        if normed_score_image_reward < 0.1:
            print("too low image reward")
            continue
        if normed_score_face < 0.1:
            print("too low face similarity")
            continue
      
        normed_face_ac_scores += normed_score_face
        normed_image_reward_ac_scores += normed_score_image_reward
      
        face_ac_scores += subed_score_face
        image_reward_ac_scores += subed_image_reward
      
        out_json["images"].append({"prompt": item["prompt"], 
                                   "scores_face": scores_face, 
                                   "scores_image_reward": scores_image_reward,
                                   "subed_score_face": subed_score_face,
                                    "subded_image_reward": subed_image_reward,
                                    "normed_score_face": normed_score_face,
                                    "normed_score_image_reward": normed_score_image_reward,
      
    with open(os.path.join(out_json_dir, f"{gen_json['id']}.json"), 'w') as f:
        json.dump(out_json, f, indent=4)
      
    return {"face_ac_scores":face_ac_scores,
            "image_reward_ac_scores":image_reward_ac_scores,
            "normed_face_ac_scores":normed_face_ac_scores,
            "normed_image_reward_ac_scores":normed_image_reward_ac_scores,
            }
```
